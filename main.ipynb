{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2df5a8a-9e8f-4d70-9a71-892342cdb616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53de6f77-86e2-4a3f-b630-71e86e467ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ライブラリのインストール\n",
    "# Readme参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0057808d-c169-4513-9e4c-68ba9324aaf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  4 03:45:06 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    On   | 00000000:19:00.0 Off |                  Off |\n",
      "| 61%   68C    P8    20W / 140W |    163MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000    On   | 00000000:1A:00.0 Off |                  Off |\n",
      "| 65%   70C    P8    24W / 140W |      8MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000    On   | 00000000:67:00.0 Off |                  Off |\n",
      "| 41%   53C    P8    15W / 140W |      8MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000    On   | 00000000:68:00.0 Off |                  Off |\n",
      "| 41%   49C    P8    16W / 140W |     77MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1814      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   3103506      C   ...ffice/program/soffice.bin      155MiB |\n",
      "|    1   N/A  N/A      1814      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      1814      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      1814      G   /usr/lib/xorg/Xorg                 63MiB |\n",
      "|    3   N/A  N/A      2120      G   /usr/bin/gnome-shell                9MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0b4206-d6c1-43f8-839e-804097859bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.7.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu11, nvidia-cuda-cupti-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, nvidia-cufft-cu11, nvidia-curand-cu11, nvidia-cusolver-cu11, nvidia-cusparse-cu11, nvidia-nccl-cu11, nvidia-nvtx-cu11, setuptools, sympy, triton, typing-extensions\n",
      "Required-by: pytorch-lightning, torchaudio, torchmetrics, torchvision\n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.22.1+cu118\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages\n",
      "Requires: numpy, pillow, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f97e5-de16-4a75-b876-6c5e745d5031",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1dce2c-124a-44ff-8b97-edea696977b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CONFIG:\n",
    "    \n",
    "    # デバックモード\n",
    "    is_debug_mode = False\n",
    "    max_class_num = 5\n",
    "\n",
    "    # 学習対象のファイル形式\n",
    "    use_mxrecord = False\n",
    "    # varidationファイルを作成するか\n",
    "    make_validation_memfiles = False\n",
    "\n",
    "    # データセットの指定(オリジナルデータセットかそうでないか)\n",
    "    is_original_dataset = True\n",
    "    ### オリジナルデータセットのコピー元：ImageFolder形式に変換する前\n",
    "    original_src_root_path = \"/data1/share/Datasets/Face/VGGFace2_frontal\"\n",
    "    ### 訓練データ\n",
    "    dataset_name = 'faces_webface_112x112' if not is_original_dataset else 'original_train_dataset'\n",
    "    dataset_dir = os.path.join(\"data\", dataset_name)\n",
    "    ### テストデータ\n",
    "    test_dataset_name = 'faces_webface_112x112' if not is_original_dataset else 'original_test_dataset'\n",
    "    test_dataset_dir = os.path.join(\"data\", test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e185db08-5ed4-4a9c-8d39-3cb0f6d33010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from convert_train_dataset import convert_to_imagefolder\n",
    "\n",
    "if not os.path.isdir(CONFIG.dataset_dir):\n",
    "    if CONFIG.dataset_name == 'faces_umd':\n",
    "        #UMD-facesをダウンロード\n",
    "        file_id = '1azhEHoJjVmifuzBVKJwl-sDbLZ-Wzp4O'\n",
    "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", f\"{CONFIG.dataset_name}.zip\", quiet=False)\n",
    "        #zip解凍\n",
    "        with zipfile.ZipFile(f\"{CONFIG.dataset_name}.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"data\")  # 任意の展開先フォルダ\n",
    "    if CONFIG.dataset_name == 'faces_webface_112x112':\n",
    "        # CASIA-webfaceをダウンロード\n",
    "        file_id = '1KxNCrXzln0lal3N4JiYl9cFOIhT78y1l'\n",
    "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", f\"{CONFIG.dataset_name}.zip\", quiet=False)\n",
    "        #zip解凍\n",
    "        with zipfile.ZipFile(f\"{CONFIG.dataset_name}.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"data\")  # 任意の展開先フォルダ\n",
    "\n",
    "if CONFIG.is_original_dataset and not os.path.isdir(CONFIG.dataset_dir) and not os.path.isdir(CONFIG.test_dataset_dir):\n",
    "    #訓練用、検証用csvファイルを読み込み、重複を削除、test_recognizer_train_list.csvを上書き保存\n",
    "    train_csv_path = 'data/target_recognizer_train_list.csv'\n",
    "    test_csv_path = 'data/eval_recognizer_train_list.csv'\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    train_paths = set(train_df['File Path'])\n",
    "    initial_len = len(test_df)\n",
    "\n",
    "    # testデータから train と重複している行を除外\n",
    "    filtered_test_df = test_df[~test_df['File Path'].isin(train_paths)]\n",
    "\n",
    "    print(f\"Removed {initial_len - len(filtered_test_df)} duplicate entries from test.\")\n",
    "\n",
    "    # 上書き保存\n",
    "    filtered_test_df.to_csv(test_csv_path, index=False)\n",
    "    print(f\"Updated {test_csv_path} successfully.\")\n",
    "\n",
    "    # オリジナルデータセットのディレクトリを作成\n",
    "    os.makedirs(CONFIG.dataset_dir, exist_ok=True)\n",
    "    os.makedirs(CONFIG.test_dataset_dir, exist_ok=True)\n",
    "\n",
    "    # データセットをImageFolder形式でコピー\n",
    "    convert_to_imagefolder(train_csv_path,CONFIG.original_src_root_path, os.path.join(CONFIG.dataset_dir,\"imgs\"))\n",
    "    convert_to_imagefolder(test_csv_path,CONFIG.original_src_root_path , os.path.join(CONFIG.test_dataset_dir,\"imgs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c716038-1840-4d41-933d-59458df28fdb",
   "metadata": {},
   "source": [
    "# rec file convert to image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9146673a-f5b4-4e79-8d60-b9274b648b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recordIO形式をImageFile形式に変換する（dataset_dir直下にimgsディレクトリが生成される）\n",
    "if not os.path.exists(f\"{CONFIG.dataset_dir}/imgs\") and not CONFIG.is_original_dataset and not CONFIG.use_mxrecord: # InsightFaceDataset\n",
    "    !python AdaFace/convert.py --rec_path {CONFIG.dataset_dir} --make_image_files \n",
    "elif not os.path.exists(f\"{CONFIG.dataset_dir}/train.rec\") and CONFIG.is_original_dataset and not CONFIG.use_mxrecord : # OriginalFaceDataset\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2fc5af4-f404-455a-ae90-623faa7ac9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_root: data\n",
      " dataset_dir: data/original_train_dataset\n",
      " dataset_img_dir:data/original_train_dataset/imgs\n",
      " data_root: data\n",
      " test_dataset_dir: data/original_test_dataset\n",
      " test_dataset_img_dir:data/original_test_dataset/imgs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# データの内容をサマライズ\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msummarize_ImageFolder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summarize_imagefolder\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43msummarize_imagefolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_img_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m summarize_imagefolder(test_dataset_img_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data1/tomoshi0514/workspace/Experience01/summarize_ImageFolder.py:27\u001b[39m, in \u001b[36msummarize_imagefolder\u001b[39m\u001b[34m(image_root)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img_path.suffix.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.bmp\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m img = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     29\u001b[39m     failed_images.append(\u001b[38;5;28mstr\u001b[39m(img_path))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_root = os.path.dirname(CONFIG.dataset_dir) \n",
    "dataset_dir = CONFIG.dataset_dir\n",
    "dataset_img_dir = os.path.join(CONFIG.dataset_dir, \"imgs\")\n",
    "test_dataset_dir = CONFIG.test_dataset_dir\n",
    "test_dataset_img_dir = os.path.join(CONFIG.test_dataset_dir, \"imgs\")\n",
    "\n",
    "print(f\" data_root: {data_root}\\n dataset_dir: {dataset_dir}\\n dataset_img_dir:{dataset_img_dir}\") \n",
    "print(f\" data_root: {data_root}\\n test_dataset_dir: {test_dataset_dir}\\n test_dataset_img_dir:{test_dataset_img_dir}\")\n",
    "\n",
    "# データの内容をサマライズ\n",
    "from summarize_ImageFolder import summarize_imagefolder\n",
    "summarize_imagefolder(dataset_img_dir)\n",
    "summarize_imagefolder(test_dataset_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600355b0-4d15-46dc-b105-50a3032e2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== debug file is created ============\n",
      " data_root: data\n",
      " dataset_dir: data/debug_dataset\n",
      " dataset_img_dir:data/debug_dataset/imgs\n",
      " data_root: data\n",
      " dataset_dir: data/debug_dataset\n",
      " dataset_img_dir:data/debug_dataset/imgs\n",
      "▶ Using dataset_path : data/debug_dataset\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "debug_dataset_dir = os.path.join(data_root, \"debug_dataset\")\n",
    "debug_img_dir = os.path.join(debug_dataset_dir, \"imgs\") \n",
    "\n",
    "# デバッグモードの場合、max_class_num分dataset_dirのimgsファイルをコピー、対象データディレクトリをdebug_datasetディレクトリに変更\n",
    "if CONFIG.is_debug_mode:\n",
    "\n",
    "    # 既存フォルダをクリア\n",
    "    if os.path.isdir(debug_dataset_dir):\n",
    "        os.remove(f\"{debug_dataset_dir}/train.idx\") if os.path.exists(f\"{debug_dataset_dir}/train.idx\") else None\n",
    "        os.remove(f\"{debug_dataset_dir}/train.lst\") if os.path.exists(f\"{debug_dataset_dir}/train.lst\") else None\n",
    "        os.remove(f\"{debug_dataset_dir}/train.rec\") if os.path.exists(f\"{debug_dataset_dir}/train.rec\") else None\n",
    "        shutil.rmtree(f\"{debug_dataset_dir}/imgs\") \n",
    "    os.makedirs(debug_dataset_dir, exist_ok=True)\n",
    "\n",
    "    # クラス名ディレクトリをソートして先頭5つ取得\n",
    "    class_dirs = sorted([\n",
    "        d for d in os.listdir(dataset_img_dir)\n",
    "        if os.path.isdir(os.path.join(dataset_img_dir, d))\n",
    "    ])[:CONFIG.max_class_num]\n",
    "\n",
    "    # 画像データをImageFolder形式としてコピー\n",
    "    for cls in class_dirs:\n",
    "        src = os.path.join(dataset_img_dir, cls)\n",
    "        dst = os.path.join(debug_img_dir, cls)\n",
    "        shutil.copytree(src, dst)\n",
    "        # もしくは高速に済ませたいなら symlink:\n",
    "        # os.symlink(src, dst, target_is_directory=True)\n",
    "\n",
    "    # dataset_dirをdebug_dataset_dirに変更\n",
    "    dataset_dir = debug_dataset_dir\n",
    "    dataset_img_dir = debug_img_dir\n",
    "\n",
    "    print(\"============== debug file is created ============\")\n",
    "    \n",
    "else:\n",
    "    print(\"==== you're using raw data (not using debug directory)\")\n",
    "\n",
    "print(f\" data_root: {data_root}\\n dataset_dir: {dataset_dir}\\n dataset_img_dir:{dataset_img_dir}\")\n",
    "\n",
    "# recordIO形式で訓練する場合、.recに変換\n",
    "if CONFIG.use_mxrecord:\n",
    "    print(\"========== create new rec file =========\")\n",
    "    !python create_train_rec.py --img_dir {dataset_img_dir} --output_dir {dataset_dir}\n",
    "    print(\".rec .lst .bin is created\")\n",
    "\n",
    "dataset_dir_name = os.path.basename(dataset_dir)\n",
    "dataset_img_dir_name = os.path.basename(dataset_img_dir)\n",
    "print(f\" data_root: {data_root}\\n dataset_dir: {dataset_dir}\\n dataset_img_dir:{dataset_img_dir}\")\n",
    "print(f\"▶ Using dataset_path : {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6479a37-46df-40ac-87ee-8bbcdf10d71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dir is already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# CASIA webfaceデータセットから対象.binファイルを事前にコピー済み\n",
    "validation_dir = os.path.join(data_root, \"validation_dataset\")\n",
    "val_dataset_dirs = [\"agedb_30\", \"calfw\", \"cfp_ff\", \"cfp_fp\", \"cplfw\", \"lfw\"]\n",
    "val_dataset_dir_name = os.path.basename(validation_dir)\n",
    "\n",
    "if all(os.path.exists(f\"{validation_dir}/{d}\") for d in val_dataset_dirs):\n",
    "    print(\"val dir is already exists\")\n",
    "else:\n",
    "    print(\"some val data is not exists : convert .bin to new val data\")\n",
    "    # 検証ファイルを構築\n",
    "    !python AdaFace/convert.py --rec_path {validation_dir} --make_validation_memfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c18a9-2404-468a-8a00-9d9d5d8ade91",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a96518-1f89-4a7b-9a4a-0ae88025dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python AdaFace/main.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a7df9",
   "metadata": {},
   "source": [
    "訓練データに使用するクラス数とクラスごとのデータ数を変えて分散の出力に影響が出るか実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9f81e-cd88-47bf-80eb-8a9612a6a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !NUM_CLASSES=$(find \"${DATA_ROOT}/${DATASET_NAME}/imgs\" -mindepth 1 -maxdepth 1 -type d | wc -l)\n",
    "# print(f\"data_root:{data_root}, dataset_dir_name:{dataset_dir_name} \")\n",
    "# # 使用中の場合は、gpuのnoを指定：CUDA_VISIBLE_DEVICES=2\n",
    "# !python AdaFace/main.py \\\n",
    "#     --data_root {data_root} \\\n",
    "#     --train_data_path {dataset_dir_name} \\\n",
    "#     --val_data_path {val_dataset_dir_name} \\\n",
    "#     --prefix ir101_ms1mv2_adaface \\\n",
    "#     --gpus 2 \\\n",
    "#     --use_16bit \\\n",
    "#     --arch ir_101 \\\n",
    "#     --batch_size 64 \\\n",
    "#     --num_workers 16 \\\n",
    "#     --epochs 5 \\\n",
    "#     --lr_milestones 12,20,24 \\\n",
    "#     --lr 0.1 \\\n",
    "#     --head adaface \\\n",
    "#     --m 0.4 \\\n",
    "#     --h 0.333 \\\n",
    "#     --low_res_augmentation_prob 0.2 \\\n",
    "#     --crop_augmentation_prob 0.2 \\\n",
    "#     --photometric_augmentation_prob 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d46c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = \"experiments/ir101_ms1mv2_adaface_07-03_1/last.ckpt\"\n",
    "# !python extract_and_analyze.py \\\n",
    "#   --ckpt_path {ckpt_path}\\\n",
    "#   --img_dir {test_dataset_img_dir} \\\n",
    "#   --batch_size 64 \\\n",
    "#   --num_workers 4 \\\n",
    "#   --device cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1: class_ratio=1.0, sample_ratio=1.0 ===\n",
      "[INFO] Copied 5 classes to data/tmp_dataset/imgs\n",
      "パースされた epochs: 1\n",
      "classnum: 5\n",
      "\n",
      "\\AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 0.01\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=2)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=2)` instead.\n",
      "  rank_zero_deprecation(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/lightning_lite/plugins/environments/slurm.py:167: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python AdaFace/main.py --data_root data --train_data_path t ...\n",
      "  rank_zero_warn(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/native_amp.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "[rank: 0] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パースされた epochs: 1\n",
      "classnum: 5\n",
      "\n",
      "\\AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 0.01\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Global seed set to 42\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/native_amp.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "[rank: 1] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train dataset\n",
      "creating train dataset\n",
      "creating val dataset\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "creating val dataset\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | model              | Backbone         | 65.2 M\n",
      "1 | head               | AdaFace          | 2.6 K \n",
      "2 | cross_entropy_loss | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------------\n",
      "65.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.2 M    Total params\n",
      "130.307   Total estimated model params size (MB)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 16/16 [00:03<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/tomoshi0514/workspace/Experience01/AdaFace/utils.py:64: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = torch.ByteStorage.from_buffer(buffer)\n",
      "/data1/tomoshi0514/workspace/Experience01/AdaFace/utils.py:64: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = torch.ByteStorage.from_buffer(buffer)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('agedb_30_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('epoch', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/972 [00:00<?, ?it/s]                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W704 03:36:28.329148534 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W704 03:36:28.375686367 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/972 [00:00<10:45,  1.51it/s, loss=28.8, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|          | 2/972 [00:01<09:13,  1.75it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   0%|          | 3/972 [00:01<06:53,  2.34it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   0%|          | 4/972 [00:01<05:43,  2.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 5/972 [00:01<05:00,  3.22it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 6/972 [00:01<04:33,  3.53it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 7/972 [00:01<04:12,  3.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 8/972 [00:01<03:57,  4.06it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 9/972 [00:02<03:45,  4.27it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 10/972 [00:02<03:36,  4.45it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 11/972 [00:02<03:28,  4.61it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|          | 12/972 [00:02<03:21,  4.75it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|▏         | 13/972 [00:02<03:16,  4.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   1%|▏         | 14/972 [00:02<03:11,  5.00it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 15/972 [00:02<03:07,  5.10it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 16/972 [00:03<03:03,  5.20it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 17/972 [00:03<03:00,  5.28it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 18/972 [00:03<02:58,  5.36it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 19/972 [00:03<02:55,  5.43it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 20/972 [00:03<02:53,  5.50it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 21/972 [00:03<02:51,  5.55it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 22/972 [00:03<02:49,  5.61it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 23/972 [00:04<02:47,  5.67it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   2%|▏         | 24/972 [00:04<02:45,  5.72it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 25/972 [00:04<02:44,  5.77it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 26/972 [00:04<02:42,  5.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 27/972 [00:04<02:41,  5.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 28/972 [00:04<02:40,  5.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 29/972 [00:04<02:39,  5.93it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 30/972 [00:05<02:38,  5.96it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 31/972 [00:05<02:37,  5.99it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 32/972 [00:05<02:36,  6.02it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 33/972 [00:05<02:35,  6.05it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   3%|▎         | 34/972 [00:05<02:34,  6.08it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▎         | 35/972 [00:05<02:33,  6.11it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▎         | 36/972 [00:05<02:32,  6.13it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 37/972 [00:06<02:31,  6.15it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 38/972 [00:06<02:31,  6.17it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 39/972 [00:06<02:30,  6.20it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 40/972 [00:06<02:29,  6.22it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 41/972 [00:06<02:29,  6.24it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 42/972 [00:06<02:28,  6.25it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   4%|▍         | 43/972 [00:06<02:28,  6.27it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▍         | 44/972 [00:07<02:27,  6.29it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▍         | 45/972 [00:07<02:27,  6.30it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▍         | 46/972 [00:07<02:26,  6.32it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▍         | 47/972 [00:07<02:26,  6.33it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▍         | 48/972 [00:07<02:25,  6.35it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▌         | 49/972 [00:07<02:25,  6.36it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▌         | 50/972 [00:07<02:24,  6.38it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▌         | 51/972 [00:07<02:24,  6.39it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▌         | 52/972 [00:08<02:23,  6.40it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   5%|▌         | 53/972 [00:08<02:23,  6.42it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 54/972 [00:08<02:22,  6.43it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 55/972 [00:08<02:22,  6.44it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 56/972 [00:08<02:21,  6.45it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 57/972 [00:08<02:21,  6.46it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 58/972 [00:08<02:21,  6.47it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 59/972 [00:09<02:20,  6.48it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▌         | 60/972 [00:09<02:20,  6.49it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▋         | 61/972 [00:09<02:20,  6.50it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▋         | 62/972 [00:09<02:19,  6.51it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   6%|▋         | 63/972 [00:09<02:19,  6.52it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 64/972 [00:09<02:19,  6.53it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 65/972 [00:09<02:18,  6.54it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 66/972 [00:10<02:18,  6.55it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 67/972 [00:10<02:18,  6.55it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 68/972 [00:10<02:17,  6.56it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 69/972 [00:10<02:17,  6.57it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 70/972 [00:10<02:17,  6.58it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 71/972 [00:10<02:16,  6.58it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   7%|▋         | 72/972 [00:10<02:16,  6.59it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 73/972 [00:11<02:16,  6.60it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 74/972 [00:11<02:15,  6.60it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 75/972 [00:11<02:15,  6.61it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 76/972 [00:11<02:15,  6.62it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 77/972 [00:11<02:15,  6.62it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 78/972 [00:11<02:14,  6.63it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 79/972 [00:11<02:14,  6.63it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 80/972 [00:12<02:14,  6.64it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 81/972 [00:12<02:14,  6.65it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   8%|▊         | 82/972 [00:12<02:13,  6.65it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▊         | 83/972 [00:12<02:13,  6.66it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▊         | 84/972 [00:12<02:13,  6.66it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▊         | 85/972 [00:12<02:13,  6.67it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 86/972 [00:12<02:12,  6.67it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 87/972 [00:13<02:12,  6.68it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 88/972 [00:13<02:12,  6.68it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 89/972 [00:13<02:12,  6.68it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 90/972 [00:13<02:11,  6.69it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 91/972 [00:13<02:11,  6.69it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:   9%|▉         | 92/972 [00:13<02:11,  6.70it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|▉         | 93/972 [00:13<02:11,  6.70it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|▉         | 94/972 [00:14<02:10,  6.71it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|▉         | 95/972 [00:14<02:10,  6.71it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|▉         | 96/972 [00:14<02:10,  6.71it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|▉         | 97/972 [00:14<02:10,  6.72it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|█         | 98/972 [00:14<02:10,  6.72it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|█         | 99/972 [00:14<02:09,  6.72it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|█         | 100/972 [00:14<02:09,  6.73it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|█         | 101/972 [00:15<02:09,  6.73it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  10%|█         | 102/972 [00:15<02:09,  6.73it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 103/972 [00:15<02:08,  6.74it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 104/972 [00:15<02:08,  6.74it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 105/972 [00:15<02:08,  6.74it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 106/972 [00:15<02:08,  6.75it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 107/972 [00:15<02:08,  6.75it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 108/972 [00:15<02:07,  6.75it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█         | 109/972 [00:16<02:07,  6.76it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█▏        | 110/972 [00:16<02:07,  6.76it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  11%|█▏        | 111/972 [00:16<02:07,  6.76it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 112/972 [00:16<02:07,  6.77it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 113/972 [00:16<02:06,  6.77it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 114/972 [00:16<02:06,  6.77it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 115/972 [00:16<02:06,  6.77it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 116/972 [00:17<02:06,  6.78it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 117/972 [00:17<02:06,  6.78it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 118/972 [00:17<02:05,  6.78it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 119/972 [00:17<02:05,  6.78it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 120/972 [00:17<02:05,  6.78it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 121/972 [00:17<02:05,  6.79it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 122/972 [00:17<02:05,  6.79it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 123/972 [00:18<02:05,  6.79it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 124/972 [00:18<02:04,  6.79it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 125/972 [00:18<02:04,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 126/972 [00:18<02:04,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 127/972 [00:18<02:04,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 128/972 [00:18<02:04,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 129/972 [00:18<02:03,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 130/972 [00:19<02:03,  6.80it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 131/972 [00:19<02:03,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▎        | 132/972 [00:19<02:03,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▎        | 133/972 [00:19<02:03,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 134/972 [00:19<02:03,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 135/972 [00:19<02:02,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 136/972 [00:19<02:02,  6.81it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 137/972 [00:20<02:02,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 138/972 [00:20<02:02,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 139/972 [00:20<02:02,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 140/972 [00:20<02:01,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 141/972 [00:20<02:01,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 142/972 [00:20<02:01,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 143/972 [00:20<02:01,  6.82it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 144/972 [00:21<02:01,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 145/972 [00:21<02:01,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 146/972 [00:21<02:00,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 147/972 [00:21<02:00,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 148/972 [00:21<02:00,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 149/972 [00:21<02:00,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 150/972 [00:21<02:00,  6.83it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 151/972 [00:22<02:00,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 152/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 153/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 154/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 155/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 156/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 157/972 [00:22<01:59,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▋        | 158/972 [00:23<01:58,  6.84it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▋        | 159/972 [00:23<01:58,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  16%|█▋        | 160/972 [00:23<01:58,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 161/972 [00:23<01:58,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 162/972 [00:23<01:58,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 163/972 [00:23<01:58,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 164/972 [00:23<01:57,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 165/972 [00:24<01:57,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 166/972 [00:24<01:57,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 167/972 [00:24<01:57,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 168/972 [00:24<01:57,  6.85it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 169/972 [00:24<01:57,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 170/972 [00:24<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 171/972 [00:24<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 172/972 [00:25<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 173/972 [00:25<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 174/972 [00:25<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 175/972 [00:25<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 176/972 [00:25<01:56,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 177/972 [00:25<01:55,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 178/972 [00:25<01:55,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 179/972 [00:26<01:55,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▊        | 180/972 [00:26<01:55,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▊        | 181/972 [00:26<01:55,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▊        | 182/972 [00:26<01:55,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 183/972 [00:26<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 184/972 [00:26<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 185/972 [00:26<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 186/972 [00:27<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 187/972 [00:27<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 188/972 [00:27<01:54,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 189/972 [00:27<01:53,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 190/972 [00:27<01:53,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 191/972 [00:27<01:53,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 192/972 [00:27<01:53,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 193/972 [00:28<01:53,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 194/972 [00:28<01:53,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|██        | 195/972 [00:28<01:53,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|██        | 196/972 [00:28<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|██        | 197/972 [00:28<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|██        | 198/972 [00:28<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  20%|██        | 199/972 [00:28<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 200/972 [00:29<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 201/972 [00:29<01:52,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 202/972 [00:29<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 203/972 [00:29<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 204/972 [00:29<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 205/972 [00:29<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██        | 206/972 [00:29<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██▏       | 207/972 [00:30<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  21%|██▏       | 208/972 [00:30<01:51,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 209/972 [00:30<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 210/972 [00:30<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 211/972 [00:30<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 212/972 [00:30<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 213/972 [00:30<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 214/972 [00:31<01:50,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 215/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 216/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 217/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 218/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 219/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 220/972 [00:31<01:49,  6.88it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 221/972 [00:32<01:49,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 222/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 223/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 224/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 225/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 226/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 227/972 [00:32<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 228/972 [00:33<01:48,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▎       | 229/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▎       | 230/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 231/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 232/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 233/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 234/972 [00:33<01:47,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 235/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 236/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 237/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 238/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 239/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 240/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 241/972 [00:34<01:46,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 242/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 243/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 244/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 245/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 246/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 247/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 248/972 [00:35<01:45,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 249/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 250/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 251/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 252/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 253/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 254/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 255/972 [00:36<01:44,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▋       | 256/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  26%|██▋       | 257/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 258/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 259/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 260/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 261/972 [00:37<01:43,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 262/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 263/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 264/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 265/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 266/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 267/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 268/972 [00:38<01:42,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 269/972 [00:39<01:41,  6.89it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 270/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 271/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 272/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 273/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 274/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 275/972 [00:39<01:41,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 276/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 277/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▊       | 278/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▊       | 279/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 280/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 281/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 282/972 [00:40<01:40,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 283/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 284/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 285/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 286/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 287/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 288/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 289/972 [00:41<01:39,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 290/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 291/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|███       | 292/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|███       | 293/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|███       | 294/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|███       | 295/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  30%|███       | 296/972 [00:42<01:38,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 297/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 298/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 299/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 300/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 301/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 302/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███       | 303/972 [00:43<01:37,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███▏      | 304/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███▏      | 305/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  31%|███▏      | 306/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 307/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 308/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 309/972 [00:44<01:36,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 310/972 [00:44<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 311/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 312/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 313/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 314/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 315/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 316/972 [00:45<01:35,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 317/972 [00:45<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 318/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 319/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 320/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 321/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 322/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 323/972 [00:46<01:34,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 324/972 [00:46<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 325/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▎      | 326/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▎      | 327/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▎      | 328/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 329/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 330/972 [00:47<01:33,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 331/972 [00:47<01:32,  6.90it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 332/972 [00:48<01:33,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 333/972 [00:48<01:33,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 334/972 [00:48<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 335/972 [00:48<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 336/972 [00:48<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 337/972 [00:49<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 338/972 [00:49<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 339/972 [00:49<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 340/972 [00:49<01:32,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 341/972 [00:49<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 342/972 [00:49<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 343/972 [00:49<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 344/972 [00:50<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 345/972 [00:50<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 346/972 [00:50<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 347/972 [00:50<01:31,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 348/972 [00:50<01:30,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 349/972 [00:50<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 350/972 [00:50<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 351/972 [00:51<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 352/972 [00:51<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▋      | 353/972 [00:51<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  36%|███▋      | 354/972 [00:51<01:30,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 355/972 [00:51<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 356/972 [00:51<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 357/972 [00:51<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 358/972 [00:52<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 359/972 [00:52<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 360/972 [00:52<01:29,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 361/972 [00:52<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 362/972 [00:52<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 363/972 [00:52<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 364/972 [00:53<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 365/972 [00:53<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 366/972 [00:53<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 367/972 [00:53<01:28,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 368/972 [00:53<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 369/972 [00:53<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 370/972 [00:53<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 371/972 [00:54<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 372/972 [00:54<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 373/972 [00:54<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 374/972 [00:54<01:27,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▊      | 375/972 [00:54<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▊      | 376/972 [00:54<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 377/972 [00:54<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 378/972 [00:55<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 379/972 [00:55<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 380/972 [00:55<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 381/972 [00:55<01:26,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 382/972 [00:55<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 383/972 [00:55<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 384/972 [00:55<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 385/972 [00:56<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 386/972 [00:56<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 387/972 [00:56<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 388/972 [00:56<01:25,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|████      | 389/972 [00:56<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|████      | 390/972 [00:56<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|████      | 391/972 [00:56<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|████      | 392/972 [00:57<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  40%|████      | 393/972 [00:57<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 394/972 [00:57<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 395/972 [00:57<01:24,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 396/972 [00:57<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 397/972 [00:57<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 398/972 [00:57<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 399/972 [00:58<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████      | 400/972 [00:58<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 401/972 [00:58<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 402/972 [00:58<01:23,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 403/972 [00:58<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 404/972 [00:58<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 405/972 [00:58<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 406/972 [00:59<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 407/972 [00:59<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 408/972 [00:59<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 409/972 [00:59<01:22,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 410/972 [00:59<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 411/972 [00:59<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 412/972 [01:00<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 413/972 [01:00<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 414/972 [01:00<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 415/972 [01:00<01:21,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 416/972 [01:00<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 417/972 [01:00<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 418/972 [01:00<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 419/972 [01:01<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 420/972 [01:01<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 421/972 [01:01<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 422/972 [01:01<01:20,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▎     | 423/972 [01:01<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▎     | 424/972 [01:01<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▎     | 425/972 [01:01<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 426/972 [01:02<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 427/972 [01:02<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 428/972 [01:02<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 429/972 [01:02<01:19,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 430/972 [01:02<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 431/972 [01:02<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 432/972 [01:02<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 433/972 [01:03<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 434/972 [01:03<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 435/972 [01:03<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 436/972 [01:03<01:18,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 437/972 [01:03<01:17,  6.87it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 438/972 [01:03<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 439/972 [01:03<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 440/972 [01:04<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 441/972 [01:04<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 442/972 [01:04<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 443/972 [01:04<01:17,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 444/972 [01:04<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 445/972 [01:04<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 446/972 [01:04<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 447/972 [01:05<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 448/972 [01:05<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 449/972 [01:05<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▋     | 450/972 [01:05<01:16,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  46%|████▋     | 451/972 [01:05<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 452/972 [01:05<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 453/972 [01:06<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 454/972 [01:06<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 455/972 [01:06<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 456/972 [01:06<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 457/972 [01:06<01:15,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 458/972 [01:06<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 459/972 [01:06<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 460/972 [01:07<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 461/972 [01:07<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 462/972 [01:07<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 463/972 [01:07<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 464/972 [01:07<01:14,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 465/972 [01:07<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 466/972 [01:07<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 467/972 [01:08<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 468/972 [01:08<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 469/972 [01:08<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 470/972 [01:08<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 471/972 [01:08<01:13,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▊     | 472/972 [01:08<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▊     | 473/972 [01:08<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 474/972 [01:09<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 475/972 [01:09<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 476/972 [01:09<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 477/972 [01:09<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 478/972 [01:09<01:12,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 479/972 [01:09<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 480/972 [01:09<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 481/972 [01:10<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 482/972 [01:10<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 483/972 [01:10<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 484/972 [01:10<01:11,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 485/972 [01:10<01:10,  6.86it/s, loss=28.8, v_num=0]\n",
      "Epoch 0:  50%|█████     | 486/972 [01:10<01:10,  6.87it/s, loss=28.8, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('cfp_fp_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('lfw_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('cplfw_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('calfw_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 486/972 [01:15<01:15,  6.44it/s, loss=28.8, v_num=0]\n",
      "                                                                          \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 487/972 [01:16<01:16,  6.36it/s, loss=28.2, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  50%|█████     | 488/972 [01:17<01:16,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  50%|█████     | 489/972 [01:17<01:16,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  50%|█████     | 490/972 [01:17<01:16,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 491/972 [01:17<01:15,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 492/972 [01:17<01:15,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 493/972 [01:17<01:15,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 494/972 [01:17<01:15,  6.33it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 495/972 [01:18<01:15,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 496/972 [01:18<01:15,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 497/972 [01:18<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████     | 498/972 [01:18<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 499/972 [01:18<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 500/972 [01:18<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 501/972 [01:19<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 502/972 [01:19<01:14,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 503/972 [01:19<01:13,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 504/972 [01:19<01:13,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 505/972 [01:19<01:13,  6.34it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 506/972 [01:19<01:13,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 507/972 [01:19<01:13,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 508/972 [01:20<01:13,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 509/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 510/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 511/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 512/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 513/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 514/972 [01:20<01:12,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 515/972 [01:21<01:11,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 516/972 [01:21<01:11,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 517/972 [01:21<01:11,  6.35it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 518/972 [01:21<01:11,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 519/972 [01:21<01:11,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 520/972 [01:21<01:11,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▎    | 521/972 [01:21<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▎    | 522/972 [01:22<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 523/972 [01:22<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 524/972 [01:22<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 525/972 [01:22<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 526/972 [01:22<01:10,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 527/972 [01:22<01:09,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 528/972 [01:22<01:09,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 529/972 [01:23<01:09,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 530/972 [01:23<01:09,  6.36it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 531/972 [01:23<01:09,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 532/972 [01:23<01:09,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 533/972 [01:23<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 534/972 [01:23<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 535/972 [01:24<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 536/972 [01:24<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 537/972 [01:24<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 538/972 [01:24<01:08,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 539/972 [01:24<01:07,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 540/972 [01:24<01:07,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 541/972 [01:24<01:07,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 542/972 [01:25<01:07,  6.37it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 543/972 [01:25<01:07,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 544/972 [01:25<01:07,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 545/972 [01:25<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 546/972 [01:25<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▋    | 547/972 [01:25<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▋    | 548/972 [01:25<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  56%|█████▋    | 549/972 [01:26<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 550/972 [01:26<01:06,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 551/972 [01:26<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 552/972 [01:26<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 553/972 [01:26<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 554/972 [01:26<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 555/972 [01:26<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 556/972 [01:27<01:05,  6.38it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 557/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 558/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 559/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 560/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 561/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 562/972 [01:27<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 563/972 [01:28<01:04,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 564/972 [01:28<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 565/972 [01:28<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 566/972 [01:28<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 567/972 [01:28<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 568/972 [01:28<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 569/972 [01:29<01:03,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 570/972 [01:29<01:02,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 571/972 [01:29<01:02,  6.39it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 572/972 [01:29<01:02,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 573/972 [01:29<01:02,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 574/972 [01:29<01:02,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 575/972 [01:29<01:02,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 576/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 577/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 578/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 579/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 580/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 581/972 [01:30<01:01,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 582/972 [01:30<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 583/972 [01:31<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|██████    | 584/972 [01:31<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|██████    | 585/972 [01:31<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|██████    | 586/972 [01:31<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|██████    | 587/972 [01:31<01:00,  6.40it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  60%|██████    | 588/972 [01:31<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 589/972 [01:31<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 590/972 [01:32<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 591/972 [01:32<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 592/972 [01:32<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 593/972 [01:32<00:59,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 594/972 [01:32<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████    | 595/972 [01:32<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 596/972 [01:32<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 597/972 [01:33<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 598/972 [01:33<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 599/972 [01:33<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 600/972 [01:33<00:58,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 601/972 [01:33<00:57,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 602/972 [01:33<00:57,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 603/972 [01:34<00:57,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 604/972 [01:34<00:57,  6.41it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 605/972 [01:34<00:57,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 606/972 [01:34<00:57,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 607/972 [01:34<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 608/972 [01:34<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 609/972 [01:34<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 610/972 [01:35<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 611/972 [01:35<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 612/972 [01:35<00:56,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 613/972 [01:35<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 614/972 [01:35<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 615/972 [01:35<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 616/972 [01:35<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 617/972 [01:36<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▎   | 618/972 [01:36<00:55,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▎   | 619/972 [01:36<00:54,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 620/972 [01:36<00:54,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 621/972 [01:36<00:54,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 622/972 [01:36<00:54,  6.42it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 623/972 [01:36<00:54,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 624/972 [01:37<00:54,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 625/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 626/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 627/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 628/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 629/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 630/972 [01:37<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 631/972 [01:38<00:53,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 632/972 [01:38<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 633/972 [01:38<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 634/972 [01:38<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 635/972 [01:38<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 636/972 [01:38<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 637/972 [01:39<00:52,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 638/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 639/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 640/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 641/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 642/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 643/972 [01:39<00:51,  6.43it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 644/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 645/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 646/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 647/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 648/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 649/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 650/972 [01:40<00:50,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 651/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 652/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 653/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 654/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 655/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 656/972 [01:41<00:49,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 657/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 658/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 659/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 660/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 661/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 662/972 [01:42<00:48,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 663/972 [01:42<00:47,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 664/972 [01:43<00:47,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 665/972 [01:43<00:47,  6.44it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 666/972 [01:43<00:47,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 667/972 [01:43<00:47,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 668/972 [01:43<00:47,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 669/972 [01:43<00:47,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 670/972 [01:43<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 671/972 [01:44<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 672/972 [01:44<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 673/972 [01:44<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 674/972 [01:44<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 675/972 [01:44<00:46,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 676/972 [01:44<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 677/972 [01:44<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 678/972 [01:45<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 679/972 [01:45<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 680/972 [01:45<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|███████   | 681/972 [01:45<00:45,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|███████   | 682/972 [01:45<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|███████   | 683/972 [01:45<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|███████   | 684/972 [01:45<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  70%|███████   | 685/972 [01:46<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 686/972 [01:46<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 687/972 [01:46<00:44,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 688/972 [01:46<00:43,  6.45it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 689/972 [01:46<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 690/972 [01:46<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 691/972 [01:47<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████   | 692/972 [01:47<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 693/972 [01:47<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 694/972 [01:47<00:43,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 695/972 [01:47<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 696/972 [01:47<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 697/972 [01:47<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 698/972 [01:48<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 699/972 [01:48<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 700/972 [01:48<00:42,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 701/972 [01:48<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 702/972 [01:48<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 703/972 [01:48<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 704/972 [01:48<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 705/972 [01:49<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 706/972 [01:49<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 707/972 [01:49<00:41,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 708/972 [01:49<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 709/972 [01:49<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 710/972 [01:49<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 711/972 [01:50<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 712/972 [01:50<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 713/972 [01:50<00:40,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 714/972 [01:50<00:39,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 715/972 [01:50<00:39,  6.46it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 716/972 [01:50<00:39,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 717/972 [01:50<00:39,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 718/972 [01:51<00:39,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 719/972 [01:51<00:39,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 720/972 [01:51<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 721/972 [01:51<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 722/972 [01:51<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 723/972 [01:51<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 724/972 [01:51<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 725/972 [01:52<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 726/972 [01:52<00:38,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 727/972 [01:52<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 728/972 [01:52<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 729/972 [01:52<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 730/972 [01:52<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 731/972 [01:52<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 732/972 [01:53<00:37,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 733/972 [01:53<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 734/972 [01:53<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 735/972 [01:53<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 736/972 [01:53<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 737/972 [01:53<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 738/972 [01:54<00:36,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 739/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 740/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 741/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 742/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 743/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 744/972 [01:54<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 745/972 [01:55<00:35,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 746/972 [01:55<00:34,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 747/972 [01:55<00:34,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 748/972 [01:55<00:34,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 749/972 [01:55<00:34,  6.47it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 750/972 [01:55<00:34,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 751/972 [01:55<00:34,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 752/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 753/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 754/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 755/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 756/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 757/972 [01:56<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 758/972 [01:57<00:33,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 759/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 760/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 761/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 762/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 763/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 764/972 [01:57<00:32,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 765/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 766/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 767/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 768/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 769/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 770/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 771/972 [01:58<00:31,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 772/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 773/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 774/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 775/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 776/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 777/972 [01:59<00:30,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|████████  | 778/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|████████  | 779/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|████████  | 780/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|████████  | 781/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  80%|████████  | 782/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 783/972 [02:00<00:29,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 784/972 [02:00<00:28,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 785/972 [02:01<00:28,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 786/972 [02:01<00:28,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 787/972 [02:01<00:28,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 788/972 [02:01<00:28,  6.48it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████  | 789/972 [02:01<00:28,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 790/972 [02:01<00:28,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 791/972 [02:01<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 792/972 [02:02<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 793/972 [02:02<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 794/972 [02:02<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 795/972 [02:02<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 796/972 [02:02<00:27,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 797/972 [02:02<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 798/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 799/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 800/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 801/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 802/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 803/972 [02:03<00:26,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 804/972 [02:03<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 805/972 [02:04<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 806/972 [02:04<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 807/972 [02:04<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 808/972 [02:04<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 809/972 [02:04<00:25,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 810/972 [02:04<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 811/972 [02:04<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 812/972 [02:05<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 813/972 [02:05<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 814/972 [02:05<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 815/972 [02:05<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 816/972 [02:05<00:24,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 817/972 [02:05<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 818/972 [02:06<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 819/972 [02:06<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 820/972 [02:06<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 821/972 [02:06<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 822/972 [02:06<00:23,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 823/972 [02:06<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 824/972 [02:06<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 825/972 [02:07<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 826/972 [02:07<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 827/972 [02:07<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 828/972 [02:07<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 829/972 [02:07<00:22,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 830/972 [02:07<00:21,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 831/972 [02:07<00:21,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 832/972 [02:08<00:21,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 833/972 [02:08<00:21,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 834/972 [02:08<00:21,  6.49it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 835/972 [02:08<00:21,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 836/972 [02:08<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 837/972 [02:08<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 838/972 [02:09<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 839/972 [02:09<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 840/972 [02:09<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 841/972 [02:09<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 842/972 [02:09<00:20,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 843/972 [02:09<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 844/972 [02:09<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 845/972 [02:10<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 846/972 [02:10<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 847/972 [02:10<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 848/972 [02:10<00:19,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 849/972 [02:10<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 850/972 [02:10<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 851/972 [02:10<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 852/972 [02:11<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 853/972 [02:11<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 854/972 [02:11<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 855/972 [02:11<00:18,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 856/972 [02:11<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 857/972 [02:11<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 858/972 [02:12<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 859/972 [02:12<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 860/972 [02:12<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 861/972 [02:12<00:17,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 862/972 [02:12<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 863/972 [02:12<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 864/972 [02:12<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 865/972 [02:13<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 866/972 [02:13<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 867/972 [02:13<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 868/972 [02:13<00:16,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 869/972 [02:13<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 870/972 [02:13<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 871/972 [02:13<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 872/972 [02:14<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 873/972 [02:14<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 874/972 [02:14<00:15,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 875/972 [02:14<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 876/972 [02:14<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 877/972 [02:14<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 878/972 [02:15<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 879/972 [02:15<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 880/972 [02:15<00:14,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 881/972 [02:15<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 882/972 [02:15<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 883/972 [02:15<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 884/972 [02:15<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 885/972 [02:16<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 886/972 [02:16<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 887/972 [02:16<00:13,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 888/972 [02:16<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 889/972 [02:16<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 890/972 [02:16<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 891/972 [02:17<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 892/972 [02:17<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 893/972 [02:17<00:12,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 894/972 [02:17<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 895/972 [02:17<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 896/972 [02:17<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 897/972 [02:17<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 898/972 [02:18<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 899/972 [02:18<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 900/972 [02:18<00:11,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 901/972 [02:18<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 902/972 [02:18<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 903/972 [02:18<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 904/972 [02:19<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 905/972 [02:19<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 906/972 [02:19<00:10,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 907/972 [02:19<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 908/972 [02:19<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 909/972 [02:19<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 910/972 [02:19<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 911/972 [02:20<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 912/972 [02:20<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 913/972 [02:20<00:09,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 914/972 [02:20<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 915/972 [02:20<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 916/972 [02:20<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 917/972 [02:21<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 918/972 [02:21<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 919/972 [02:21<00:08,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 920/972 [02:21<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 921/972 [02:21<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 922/972 [02:21<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 923/972 [02:21<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 924/972 [02:22<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 925/972 [02:22<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 926/972 [02:22<00:07,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 927/972 [02:22<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 928/972 [02:22<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 929/972 [02:22<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 930/972 [02:23<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 931/972 [02:23<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 932/972 [02:23<00:06,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 933/972 [02:23<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 934/972 [02:23<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 935/972 [02:23<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 936/972 [02:23<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 937/972 [02:24<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 938/972 [02:24<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 939/972 [02:24<00:05,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 940/972 [02:24<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 941/972 [02:24<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 942/972 [02:24<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 943/972 [02:24<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 944/972 [02:25<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 945/972 [02:25<00:04,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 946/972 [02:25<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 947/972 [02:25<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 948/972 [02:25<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 949/972 [02:25<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 950/972 [02:26<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 951/972 [02:26<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 952/972 [02:26<00:03,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 953/972 [02:26<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 954/972 [02:26<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 955/972 [02:26<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 956/972 [02:26<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 957/972 [02:27<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 958/972 [02:27<00:02,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 959/972 [02:27<00:01,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 960/972 [02:27<00:01,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 961/972 [02:27<00:01,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 962/972 [02:27<00:01,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 963/972 [02:28<00:01,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 964/972 [02:28<00:01,  6.50it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 965/972 [02:28<00:01,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 966/972 [02:28<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 967/972 [02:28<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 968/972 [02:28<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 969/972 [02:28<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 970/972 [02:29<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 971/972 [02:29<00:00,  6.51it/s, loss=28.2, v_num=0]\n",
      "Epoch 0: 100%|██████████| 972/972 [02:36<00:00,  6.23it/s, loss=28.2, v_num=0]\n",
      "                                                                          \u001b[Astart evaluating\n",
      "evaluating from  experiments/exp_variance_test_1_07-03_4/epoch=0-step=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Global seed set to 42\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 972/972 [02:38<00:00,  6.14it/s, loss=28.2, v_num=0]\n",
      "start evaluating\n",
      "evaluating from  experiments/exp_variance_test_1_07-03_4/epoch=0-step=1.ckpt\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n",
      "Restoring states from the checkpoint path at experiments/exp_variance_test_1_07-03_4/epoch=0-step=1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at experiments/exp_variance_test_1_07-03_4/epoch=0-step=1.ckpt\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 485/485 [01:12<00:00,  6.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('agedb_30_num_test_samples', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('cfp_fp_num_test_samples', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('lfw_num_test_samples', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('cplfw_num_test_samples', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('calfw_num_test_samples', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('epoch', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_test_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_num_test_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_test_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cfp_fp_num_test_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_test_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('lfw_num_test_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_test_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('cplfw_num_test_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_test_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('calfw_num_test_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 485/485 [01:18<00:00,  6.18it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.testing metric         DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      " agedb_30_num_test_samples            12000.0\n",
      "     agedb_30_test_acc                  0.5\n",
      "agedb_30_test_best_threshold            0.0\n",
      "   calfw_num_test_samples             12000.0\n",
      "       calfw_test_acc                   0.5\n",
      " calfw_test_best_threshold              0.0\n",
      "  cfp_fp_num_test_samples             14000.0\n",
      "      cfp_fp_test_acc                   0.5\n",
      " cfp_fp_test_best_threshold             0.0\n",
      "   cplfw_num_test_samples             12000.0\n",
      "       cplfw_test_acc                   0.5\n",
      " cplfw_test_best_threshold              0.0\n",
      "           epoch                        0.0\n",
      "    lfw_num_test_samples              12000.0\n",
      "        lfw_test_acc                    0.5\n",
      "  lfw_test_best_threshold               0.0\n",
      "          test_acc                      0.5\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W704 03:40:26.379708849 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Latest experiment directory: experiments/exp_variance_test_1_07-03_4\n",
      "Total number of features: 200\n",
      "Feature dimension: 512\n",
      "Variance Var(X): 0.146202\n",
      "[INFO] Ran extract_and_analyze.py to training data for experiment  1\n",
      "Total number of features: 85600\n",
      "Feature dimension: 512\n",
      "Variance Var(X): 0.149113\n",
      "[INFO] Ran extract_and_analyze.py to training data for experiment  1\n",
      "[INFO] Cleaned up experiment 1\n",
      "\n",
      "=== Experiment 2: class_ratio=0.5, sample_ratio=1.0 ===\n",
      "[INFO] Copied 2 classes to data/tmp_dataset/imgs\n",
      "パースされた epochs: 1\n",
      "classnum: 2\n",
      "\n",
      "\\AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 0.01\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=2)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=2)` instead.\n",
      "  rank_zero_deprecation(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/lightning_lite/plugins/environments/slurm.py:167: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python AdaFace/main.py --data_root data --train_data_path t ...\n",
      "  rank_zero_warn(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/native_amp.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "[rank: 0] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パースされた epochs: 1\n",
      "classnum: 2\n",
      "\n",
      "\\AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 0.01\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Global seed set to 42\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/native_amp.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "[rank: 1] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train dataset\n",
      "creating train dataset\n",
      "creating val datasetcreating val dataset\n",
      "\n",
      "laoding validation data memfilelaoding validation data memfile\n",
      "\n",
      "laoding validation data memfilelaoding validation data memfile\n",
      "\n",
      "laoding validation data memfilelaoding validation data memfile\n",
      "\n",
      "laoding validation data memfilelaoding validation data memfile\n",
      "\n",
      "laoding validation data memfile\n",
      "laoding validation data memfile\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | model              | Backbone         | 65.2 M\n",
      "1 | head               | AdaFace          | 1.0 K \n",
      "2 | cross_entropy_loss | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------------\n",
      "65.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.2 M    Total params\n",
      "130.304   Total estimated model params size (MB)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 16/16 [00:03<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/tomoshi0514/workspace/Experience01/AdaFace/utils.py:64: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = torch.ByteStorage.from_buffer(buffer)\n",
      "/data1/tomoshi0514/workspace/Experience01/AdaFace/utils.py:64: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = torch.ByteStorage.from_buffer(buffer)\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('agedb_30_num_val_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('epoch', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_best_threshold', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('agedb_30_num_val_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/486 [00:00<?, ?it/s]                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W704 03:43:43.509623724 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W704 03:43:43.518696828 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/486 [00:00<04:30,  1.79it/s, loss=25.4, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/485 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|          | 2/486 [00:01<04:14,  1.90it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   1%|          | 3/486 [00:01<03:11,  2.52it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   1%|          | 4/486 [00:01<02:41,  2.99it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   1%|          | 5/486 [00:01<02:22,  3.38it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   1%|          | 6/486 [00:01<02:10,  3.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   1%|▏         | 7/486 [00:01<02:01,  3.96it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   2%|▏         | 8/486 [00:01<01:54,  4.19it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   2%|▏         | 9/486 [00:02<01:48,  4.38it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   2%|▏         | 10/486 [00:02<01:44,  4.55it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   2%|▏         | 11/486 [00:02<01:41,  4.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   2%|▏         | 12/486 [00:02<01:38,  4.82it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   3%|▎         | 13/486 [00:02<01:35,  4.94it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   3%|▎         | 14/486 [00:02<01:33,  5.05it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   3%|▎         | 15/486 [00:02<01:31,  5.13it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   3%|▎         | 16/486 [00:03<01:30,  5.22it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   3%|▎         | 17/486 [00:03<01:28,  5.30it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   4%|▎         | 18/486 [00:03<01:27,  5.37it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   4%|▍         | 19/486 [00:03<01:25,  5.44it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   4%|▍         | 20/486 [00:03<01:24,  5.50it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   4%|▍         | 21/486 [00:03<01:23,  5.55it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   5%|▍         | 22/486 [00:03<01:22,  5.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   5%|▍         | 23/486 [00:04<01:21,  5.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   5%|▍         | 24/486 [00:04<01:21,  5.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   5%|▌         | 25/486 [00:04<01:20,  5.73it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   5%|▌         | 26/486 [00:04<01:19,  5.77it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   6%|▌         | 27/486 [00:04<01:19,  5.81it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   6%|▌         | 28/486 [00:04<01:18,  5.84it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   6%|▌         | 29/486 [00:04<01:17,  5.87it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   6%|▌         | 30/486 [00:05<01:17,  5.90it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   6%|▋         | 31/486 [00:05<01:16,  5.93it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   7%|▋         | 32/486 [00:05<01:16,  5.96it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   7%|▋         | 33/486 [00:05<01:15,  5.99it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   7%|▋         | 34/486 [00:05<01:15,  6.01it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   7%|▋         | 35/486 [00:05<01:14,  6.03it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   7%|▋         | 36/486 [00:05<01:14,  6.05it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   8%|▊         | 37/486 [00:06<01:13,  6.07it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   8%|▊         | 38/486 [00:06<01:13,  6.09it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   8%|▊         | 39/486 [00:06<01:13,  6.11it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   8%|▊         | 40/486 [00:06<01:12,  6.13it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   8%|▊         | 41/486 [00:06<01:12,  6.15it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   9%|▊         | 42/486 [00:06<01:12,  6.16it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   9%|▉         | 43/486 [00:06<01:11,  6.18it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   9%|▉         | 44/486 [00:07<01:11,  6.19it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   9%|▉         | 45/486 [00:07<01:11,  6.21it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:   9%|▉         | 46/486 [00:07<01:10,  6.22it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  10%|▉         | 47/486 [00:07<01:10,  6.24it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  10%|▉         | 48/486 [00:07<01:10,  6.25it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  10%|█         | 49/486 [00:07<01:09,  6.26it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  10%|█         | 50/486 [00:07<01:09,  6.27it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  10%|█         | 51/486 [00:08<01:09,  6.28it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  11%|█         | 52/486 [00:08<01:08,  6.30it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  11%|█         | 53/486 [00:08<01:08,  6.31it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  11%|█         | 54/486 [00:08<01:08,  6.32it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  11%|█▏        | 55/486 [00:08<01:08,  6.33it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 56/486 [00:08<01:07,  6.34it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 57/486 [00:08<01:07,  6.34it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 58/486 [00:09<01:07,  6.35it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 59/486 [00:09<01:07,  6.36it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 60/486 [00:09<01:06,  6.37it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 61/486 [00:09<01:06,  6.37it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 62/486 [00:09<01:06,  6.38it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 63/486 [00:09<01:06,  6.39it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 64/486 [00:10<01:05,  6.40it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  13%|█▎        | 65/486 [00:10<01:05,  6.41it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  14%|█▎        | 66/486 [00:10<01:05,  6.41it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 67/486 [00:10<01:05,  6.42it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 68/486 [00:10<01:05,  6.43it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 69/486 [00:10<01:04,  6.43it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 70/486 [00:10<01:04,  6.44it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 71/486 [00:11<01:04,  6.44it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  15%|█▍        | 72/486 [00:11<01:04,  6.45it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 73/486 [00:11<01:03,  6.46it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 74/486 [00:11<01:03,  6.46it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  15%|█▌        | 75/486 [00:11<01:03,  6.47it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 76/486 [00:11<01:03,  6.47it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 77/486 [00:11<01:03,  6.48it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  16%|█▌        | 78/486 [00:12<01:02,  6.48it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  16%|█▋        | 79/486 [00:12<01:02,  6.49it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  16%|█▋        | 80/486 [00:12<01:02,  6.49it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 81/486 [00:12<01:02,  6.50it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 82/486 [00:12<01:02,  6.50it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 83/486 [00:12<01:01,  6.51it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 84/486 [00:12<01:01,  6.51it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  17%|█▋        | 85/486 [00:13<01:01,  6.51it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 86/486 [00:13<01:01,  6.52it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 87/486 [00:13<01:01,  6.52it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 88/486 [00:13<01:01,  6.52it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 89/486 [00:13<01:00,  6.53it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  19%|█▊        | 90/486 [00:13<01:00,  6.53it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  19%|█▊        | 91/486 [00:13<01:00,  6.53it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 92/486 [00:14<01:00,  6.54it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 93/486 [00:14<01:00,  6.54it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  19%|█▉        | 94/486 [00:14<00:59,  6.54it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 95/486 [00:14<00:59,  6.55it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 96/486 [00:14<00:59,  6.55it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  20%|█▉        | 97/486 [00:14<00:59,  6.55it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  20%|██        | 98/486 [00:14<00:59,  6.56it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  20%|██        | 99/486 [00:15<00:58,  6.56it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  21%|██        | 100/486 [00:15<00:58,  6.56it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  21%|██        | 101/486 [00:15<00:58,  6.57it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  21%|██        | 102/486 [00:15<00:58,  6.57it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  21%|██        | 103/486 [00:15<00:58,  6.57it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  21%|██▏       | 104/486 [00:15<00:58,  6.57it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 105/486 [00:15<00:57,  6.58it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 106/486 [00:16<00:57,  6.58it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 107/486 [00:16<00:57,  6.58it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 108/486 [00:16<00:57,  6.59it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  22%|██▏       | 109/486 [00:16<00:57,  6.59it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 110/486 [00:16<00:57,  6.59it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 111/486 [00:16<00:56,  6.59it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 112/486 [00:16<00:56,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 113/486 [00:17<00:56,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  23%|██▎       | 114/486 [00:17<00:56,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  24%|██▎       | 115/486 [00:17<00:56,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 116/486 [00:17<00:56,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 117/486 [00:17<00:55,  6.60it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 118/486 [00:17<00:55,  6.61it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  24%|██▍       | 119/486 [00:18<00:55,  6.61it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 120/486 [00:18<00:55,  6.61it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  25%|██▍       | 121/486 [00:18<00:55,  6.61it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 122/486 [00:18<00:55,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 123/486 [00:18<00:54,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 124/486 [00:18<00:54,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 125/486 [00:18<00:54,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 126/486 [00:19<00:54,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  26%|██▌       | 127/486 [00:19<00:54,  6.62it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  26%|██▋       | 128/486 [00:19<00:54,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 129/486 [00:19<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 130/486 [00:19<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 131/486 [00:19<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 132/486 [00:19<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  27%|██▋       | 133/486 [00:20<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 134/486 [00:20<00:53,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 135/486 [00:20<00:52,  6.63it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 136/486 [00:20<00:52,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 137/486 [00:20<00:52,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  28%|██▊       | 138/486 [00:20<00:52,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  29%|██▊       | 139/486 [00:20<00:52,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 140/486 [00:21<00:52,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 141/486 [00:21<00:51,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 142/486 [00:21<00:51,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  29%|██▉       | 143/486 [00:21<00:51,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 144/486 [00:21<00:51,  6.64it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 145/486 [00:21<00:51,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  30%|███       | 146/486 [00:21<00:51,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  30%|███       | 147/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  30%|███       | 148/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  31%|███       | 149/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  31%|███       | 150/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  31%|███       | 151/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  31%|███▏      | 152/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  31%|███▏      | 153/486 [00:22<00:50,  6.65it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 154/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 155/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 156/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 157/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 158/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 159/486 [00:23<00:49,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 160/486 [00:24<00:48,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 161/486 [00:24<00:48,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  33%|███▎      | 162/486 [00:24<00:48,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  34%|███▎      | 163/486 [00:24<00:48,  6.66it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  34%|███▎      | 164/486 [00:24<00:48,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 165/486 [00:24<00:48,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 166/486 [00:24<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  34%|███▍      | 167/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 168/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 169/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  35%|███▍      | 170/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 171/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  35%|███▌      | 172/486 [00:25<00:47,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 173/486 [00:25<00:46,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 174/486 [00:26<00:46,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 175/486 [00:26<00:46,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 176/486 [00:26<00:46,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  36%|███▋      | 177/486 [00:26<00:46,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 178/486 [00:26<00:46,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 179/486 [00:26<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 180/486 [00:26<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 181/486 [00:27<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  37%|███▋      | 182/486 [00:27<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 183/486 [00:27<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 184/486 [00:27<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 185/486 [00:27<00:45,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 186/486 [00:27<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 187/486 [00:27<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  39%|███▊      | 188/486 [00:28<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 189/486 [00:28<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 190/486 [00:28<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 191/486 [00:28<00:44,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 192/486 [00:28<00:43,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 193/486 [00:28<00:43,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  40%|███▉      | 194/486 [00:29<00:43,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  40%|████      | 195/486 [00:29<00:43,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  40%|████      | 196/486 [00:29<00:43,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  41%|████      | 197/486 [00:29<00:43,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  41%|████      | 198/486 [00:29<00:43,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  41%|████      | 199/486 [00:29<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  41%|████      | 200/486 [00:29<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  41%|████▏     | 201/486 [00:30<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 202/486 [00:30<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 203/486 [00:30<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 204/486 [00:30<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 205/486 [00:30<00:42,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  42%|████▏     | 206/486 [00:30<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 207/486 [00:30<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 208/486 [00:31<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 209/486 [00:31<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 210/486 [00:31<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 211/486 [00:31<00:41,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  44%|████▎     | 212/486 [00:31<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 213/486 [00:31<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 214/486 [00:31<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 215/486 [00:32<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  44%|████▍     | 216/486 [00:32<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 217/486 [00:32<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 218/486 [00:32<00:40,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 219/486 [00:32<00:39,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 220/486 [00:32<00:39,  6.69it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  45%|████▌     | 221/486 [00:33<00:39,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 222/486 [00:33<00:39,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 223/486 [00:33<00:39,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 224/486 [00:33<00:39,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  46%|████▋     | 225/486 [00:33<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 226/486 [00:33<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 227/486 [00:33<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 228/486 [00:34<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 229/486 [00:34<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 230/486 [00:34<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 231/486 [00:34<00:38,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 232/486 [00:34<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 233/486 [00:34<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 234/486 [00:34<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  48%|████▊     | 235/486 [00:35<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  49%|████▊     | 236/486 [00:35<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 237/486 [00:35<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 238/486 [00:35<00:37,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 239/486 [00:35<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  49%|████▉     | 240/486 [00:35<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 241/486 [00:35<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  50%|████▉     | 242/486 [00:36<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  50%|█████     | 243/486 [00:36<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  50%|█████     | 244/486 [00:36<00:36,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  50%|█████     | 245/486 [00:36<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  51%|█████     | 246/486 [00:36<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  51%|█████     | 247/486 [00:36<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  51%|█████     | 248/486 [00:37<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  51%|█████     | 249/486 [00:37<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  51%|█████▏    | 250/486 [00:37<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 251/486 [00:37<00:35,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 252/486 [00:37<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 253/486 [00:37<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 254/486 [00:37<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  52%|█████▏    | 255/486 [00:38<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 256/486 [00:38<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 257/486 [00:38<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 258/486 [00:38<00:34,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 259/486 [00:38<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  53%|█████▎    | 260/486 [00:38<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  54%|█████▎    | 261/486 [00:38<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 262/486 [00:39<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 263/486 [00:39<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 264/486 [00:39<00:33,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 265/486 [00:39<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 266/486 [00:39<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  55%|█████▍    | 267/486 [00:39<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 268/486 [00:39<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  55%|█████▌    | 269/486 [00:40<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 270/486 [00:40<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 271/486 [00:40<00:32,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 272/486 [00:40<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  56%|█████▌    | 273/486 [00:40<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  56%|█████▋    | 274/486 [00:40<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 275/486 [00:41<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 276/486 [00:41<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 277/486 [00:41<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 278/486 [00:41<00:31,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 279/486 [00:41<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 280/486 [00:41<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 281/486 [00:41<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 282/486 [00:42<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 283/486 [00:42<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  58%|█████▊    | 284/486 [00:42<00:30,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  59%|█████▊    | 285/486 [00:42<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 286/486 [00:42<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 287/486 [00:42<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 288/486 [00:42<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  59%|█████▉    | 289/486 [00:43<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 290/486 [00:43<00:29,  6.70it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  60%|█████▉    | 291/486 [00:43<00:29,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  60%|██████    | 292/486 [00:43<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  60%|██████    | 293/486 [00:43<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  60%|██████    | 294/486 [00:43<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  61%|██████    | 295/486 [00:43<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  61%|██████    | 296/486 [00:44<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  61%|██████    | 297/486 [00:44<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  61%|██████▏   | 298/486 [00:44<00:28,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 299/486 [00:44<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 300/486 [00:44<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 301/486 [00:44<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 302/486 [00:45<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 303/486 [00:45<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 304/486 [00:45<00:27,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 305/486 [00:45<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 306/486 [00:45<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 307/486 [00:45<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 308/486 [00:45<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  64%|██████▎   | 309/486 [00:46<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 310/486 [00:46<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 311/486 [00:46<00:26,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 312/486 [00:46<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 313/486 [00:46<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 314/486 [00:46<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  65%|██████▍   | 315/486 [00:46<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 316/486 [00:47<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 317/486 [00:47<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  65%|██████▌   | 318/486 [00:47<00:25,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 319/486 [00:47<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 320/486 [00:47<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  66%|██████▌   | 321/486 [00:47<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 322/486 [00:48<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 323/486 [00:48<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 324/486 [00:48<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 325/486 [00:48<00:24,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 326/486 [00:48<00:23,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 327/486 [00:48<00:23,  6.71it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 328/486 [00:49<00:23,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 329/486 [00:49<00:23,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 330/486 [00:49<00:23,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 331/486 [00:49<00:23,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 332/486 [00:49<00:23,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 333/486 [00:49<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 334/486 [00:50<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 335/486 [00:50<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 336/486 [00:50<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 337/486 [00:50<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 338/486 [00:50<00:22,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 339/486 [00:50<00:22,  6.68it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 340/486 [00:50<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  70%|███████   | 341/486 [00:51<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  70%|███████   | 342/486 [00:51<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  71%|███████   | 343/486 [00:51<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  71%|███████   | 344/486 [00:51<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  71%|███████   | 345/486 [00:51<00:21,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  71%|███████   | 346/486 [00:51<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 347/486 [00:51<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 348/486 [00:52<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 349/486 [00:52<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 350/486 [00:52<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 351/486 [00:52<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 352/486 [00:52<00:20,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 353/486 [00:52<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 354/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 355/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 356/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 357/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 358/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 359/486 [00:53<00:19,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 360/486 [00:53<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 361/486 [00:54<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 362/486 [00:54<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 363/486 [00:54<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 364/486 [00:54<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 365/486 [00:54<00:18,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 366/486 [00:54<00:17,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 367/486 [00:55<00:17,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 368/486 [00:55<00:17,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 369/486 [00:55<00:17,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 370/486 [00:55<00:17,  6.67it/s, loss=25.4, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 371/486 [00:55<00:17,  6.67it/s, loss=25.4, v_num=0]start evaluating\n",
      "evaluating from  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating\n",
      "evaluating from  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 3. main.py 実行\u001b[39;00m\n\u001b[32m     53\u001b[39m cmd = [\n\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdaFace/main.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--data_root\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(data_root),\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--photometric_augmentation_prob\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# 4. 最新 experiments ディレクトリの取得\u001b[39;00m\n\u001b[32m     77\u001b[39m exp_dir = Path(\u001b[33m\"\u001b[39m\u001b[33mexperiments\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W704 03:44:39.697989590 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=18, addr=[::ffff:127.0.0.1]:59294, remote=[::ffff:127.0.0.1]:47657): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14643ee955e8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8b0e (0x146483b92b0e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae50 (0x146483b94e50 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab75a (0x146483b9575a in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x146483b8f1b9 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x1464400f3729 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x14643c76cbf4 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x146497f06609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x146497cd1353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W704 03:44:39.707808486 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank1]:[W704 03:44:40.708014795 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=18, addr=[::ffff:127.0.0.1]:59294, remote=[::ffff:127.0.0.1]:47657): Broken pipe\n",
      "Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14643ee955e8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8b0e (0x146483b92b0e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baa368 (0x146483b94368 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5babb4e (0x146483b95b4e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x146483b8f1a8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x1464400f3729 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x14643c76cbf4 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x146497f06609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x146497cd1353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W704 03:44:40.717522734 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n",
      "[rank1]:[W704 03:44:41.717769774 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=18, addr=[::ffff:127.0.0.1]:59294, remote=[::ffff:127.0.0.1]:47657): Broken pipe\n",
      "Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14643ee955e8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8b0e (0x146483b92b0e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baa368 (0x146483b94368 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5babb4e (0x146483b95b4e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x146483b8f1a8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x1464400f3729 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x14643c76cbf4 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x146497f06609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x146497cd1353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W704 03:44:41.727456065 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/data1/tomoshi0514/workspace/Experience01/AdaFace/main.py\", line 117, in <module>\n",
      "[rank1]:     main(args)\n",
      "[rank1]:   File \"/data1/tomoshi0514/workspace/Experience01/AdaFace/main.py\", line 92, in main\n",
      "[rank1]:     trainer.test(ckpt_path='best', datamodule=data_mod)\n",
      "[rank1]:   File \"/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 780, in test\n",
      "[rank1]:     return call._call_and_handle_interrupt(\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "[rank1]:     return trainer_fn(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 822, in _test_impl\n",
      "[rank1]:     self._ckpt_path = self._checkpoint_connector._set_ckpt_path(\n",
      "[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 160, in _set_ckpt_path\n",
      "[rank1]:     raise ValueError(\n",
      "[rank1]: ValueError: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
      "[rank1]:[W704 03:44:42.727705744 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=18, addr=[::ffff:127.0.0.1]:59294, remote=[::ffff:127.0.0.1]:47657): Broken pipe\n",
      "Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x14643ee955e8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8b0e (0x146483b92b0e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baa368 (0x146483b94368 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5babb4e (0x146483b95b4e in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x146483b8f1a8 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x1464400f3729 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x14643c76cbf4 in /data2/tomoshi0514/anaconda3/envs/AdafaceTest/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x146497f06609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x146497cd1353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W704 03:44:42.732222247 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# ==== パラメータ設定 ====\n",
    "data_root = Path(\"data\")\n",
    "original_dataset_path = data_root / dataset_dir_name \n",
    "original_dataset_imgs_path = original_dataset_path / \"imgs\"\n",
    "val_data_path = \"validation_dataset\"\n",
    "experiment_prefix = \"exp_variance_test\"\n",
    "tmp_dataset_path = data_root / \"tmp_dataset\"\n",
    "extract_script_path = \"extract_and_analyze.py\"\n",
    "gpu_num = 2\n",
    "\n",
    "experiment_configs = [\n",
    "    {\"class_ratio\": 1.0, \"sample_ratio\": 1.0},\n",
    "    {\"class_ratio\": 0.5, \"sample_ratio\": 1.0},\n",
    "    {\"class_ratio\": 1.0, \"sample_ratio\": 0.5},\n",
    "    {\"class_ratio\": 0.5, \"sample_ratio\": 0.5},\n",
    "]\n",
    "\n",
    "# ==== 実験のループ開始 ====\n",
    "for idx, config in enumerate(experiment_configs):\n",
    "    class_ratio = config[\"class_ratio\"]\n",
    "    sample_ratio = config[\"sample_ratio\"]\n",
    "    print(f\"\\n=== Experiment {idx+1}: class_ratio={class_ratio}, sample_ratio={sample_ratio} ===\")\n",
    "\n",
    "    # 1. tmp_datasetの作成（存在する場合は削除）\n",
    "    if tmp_dataset_path.exists():\n",
    "        shutil.rmtree(tmp_dataset_path)\n",
    "    tmp_imgs_path = tmp_dataset_path / \"imgs\"\n",
    "    tmp_imgs_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2. クラスディレクトリの取得と選択\n",
    "    class_dirs = sorted([d for d in original_dataset_imgs_path.iterdir() if d.is_dir()])\n",
    "    selected_class_dirs = random.sample(class_dirs, int(len(class_dirs) * class_ratio))\n",
    "\n",
    "    for class_dir in selected_class_dirs:\n",
    "        images = sorted(class_dir.glob(\"*.png\"))\n",
    "        selected_images = random.sample(images, max(1, int(len(images) * sample_ratio)))\n",
    "        dest_dir = tmp_imgs_path / class_dir.name\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for img_path in selected_images:\n",
    "            shutil.copy2(img_path, dest_dir / img_path.name)\n",
    "\n",
    "    print(f\"[INFO] Copied {len(selected_class_dirs)} classes to {tmp_imgs_path}\")\n",
    "\n",
    "    # 3. main.py 実行\n",
    "    cmd = [\n",
    "        \"python\", \"AdaFace/main.py\",\n",
    "        \"--data_root\", str(data_root),\n",
    "        \"--train_data_path\", tmp_dataset_path.name,\n",
    "        \"--val_data_path\", val_data_path,\n",
    "        \"--prefix\", f\"{experiment_prefix}_{idx+1}\",\n",
    "        \"--gpus\", str(gpu_num),\n",
    "        \"--use_16bit\",\n",
    "        \"--arch\", \"ir_101\",\n",
    "        \"--batch_size\", \"64\",\n",
    "        \"--num_workers\", \"4\",\n",
    "        \"--epochs\", \"6\",\n",
    "        \"--lr_milestones\", \"12,20,24\",\n",
    "        \"--lr\", \"0.1\",\n",
    "        \"--head\", \"adaface\",\n",
    "        \"--m\", \"0.4\",\n",
    "        \"--h\", \"0.333\",\n",
    "        \"--low_res_augmentation_prob\", \"0.2\",\n",
    "        \"--crop_augmentation_prob\", \"0.2\",\n",
    "        \"--photometric_augmentation_prob\", \"0.2\"\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # 4. 最新 experiments ディレクトリの取得\n",
    "    exp_dir = Path(\"experiments\")\n",
    "    latest_exp_dir = max(exp_dir.glob(\"*\"), key=os.path.getmtime)\n",
    "    ckpt_path = latest_exp_dir / \"last.ckpt\"\n",
    "    print(f\"[INFO] Latest experiment directory: {latest_exp_dir}\")\n",
    "\n",
    "    # 5. extract_and_analyze.py を train データに対し実行\n",
    "    extract2train_cmd = [\n",
    "        \"python\", extract_script_path,\n",
    "        \"--ckpt_path\", str(ckpt_path),\n",
    "        \"--img_dir\", str(tmp_imgs_path),\n",
    "        \"--batch_size\", \"64\",\n",
    "        \"--num_workers\", \"4\"\n",
    "    ]\n",
    "    subprocess.run(extract2train_cmd, check=True)\n",
    "    print(f\"[INFO] Ran extract_and_analyze.py to training data for experiment  {idx+1}\")\n",
    "\n",
    "    # 6. extract_and_analyze.py を testデータに対し実行\n",
    "    extract2test_cmd = [\n",
    "        \"python\", extract_script_path,\n",
    "        \"--ckpt_path\", str(ckpt_path),\n",
    "        \"--img_dir\", test_dataset_img_dir,\n",
    "        \"--batch_size\", \"64\",\n",
    "        \"--num_workers\", \"4\"\n",
    "    ]\n",
    "    subprocess.run(extract2test_cmd, check=True)\n",
    "\n",
    "    print(f\"[INFO] Ran extract_and_analyze.py to training data for experiment  {idx+1}\")\n",
    "    # 7. 後始末：experimentログとtmpデータ削除\n",
    "    shutil.rmtree(latest_exp_dir)\n",
    "    shutil.rmtree(tmp_dataset_path)\n",
    "    print(f\"[INFO] Cleaned up experiment {idx+1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7ba9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683be94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdafaceTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
